{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ismail Kadare - Gjakftohtesia\n",
    "\n",
    "This notebook builds a character-wise RNN trained on \"Gjakftohtesia\" of the albanian author Ismail Kadare. It'll be used to generate a new chapter.\n",
    "\n",
    "This network is based on https://github.com/udacity/deep-learning/tree/master/intro-to-rnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Ismail Kadare - Gjakftohtesia.txt', 'r', encoding=\"iso-8859-1\") as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab ['\\n', '\\x0c', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '\\x82', '\\x86', '\\x88', '\\x89', '\\x8a', '\\x8d', '\\x8e', '\\x8f', '\\x90', '\\x91', '\\x93', '\\x94', '\\x95', '\\x97', '\\x98', '\\x99', '\\x9a', '\\x9d', '\\x9e', '\\x9f', '¡', '£', '¤', '¥', '§', 'Ç', 'È', 'Ñ', 'ã', 'å', 'æ', 'è', 'ë', 'ó']\n",
      "vocab_to_int {'A': 32, 'c': 65, 't': 82, 'h': 70, 'T': 51, '\\x82': 90, 'R': 49, 'z': 88, 'Ç': 115, 'D': 35, '\"': 4, 'G': 38, '\\\\': 59, '\\x86': 91, '£': 111, '\\x97': 103, '0': 17, '\\x91': 99, '-': 14, '~': 89, 'ó': 123, '1': 18, '\\x95': 102, 'æ': 120, 'è': 121, '\\x8e': 96, 'q': 79, ':': 27, 'H': 39, '\\x94': 101, '5': 22, '*': 11, 'p': 78, 'È': 116, 's': 81, ';': 28, 'u': 83, 'o': 77, 'r': 80, 'X': 55, '\\x9a': 106, '\\x8f': 97, '\\x89': 93, 'l': 74, 'I': 40, '6': 23, '%': 6, '3': 20, 'Z': 57, 'ë': 122, '§': 114, 'b': 64, 'U': 52, 'k': 73, '_': 62, 'w': 85, '¥': 113, '#': 5, 'J': 41, 'S': 50, '\\x8d': 95, '&': 7, 'C': 34, \"'\": 8, 'm': 75, 'å': 119, 'B': 33, '<': 29, '>': 30, 'v': 84, '\\x9d': 107, '+': 12, ']': 60, '/': 16, ' ': 2, 'j': 72, '!': 3, 'F': 37, '\\x9f': 109, 'V': 53, 'i': 71, 'N': 45, 'x': 86, '\\x8a': 94, 'W': 54, 'y': 87, '\\x90': 98, 'K': 42, 'E': 36, 'g': 69, '\\x0c': 1, 'f': 68, 'M': 44, 'n': 76, 'Ñ': 117, '7': 24, 'O': 46, '4': 21, '(': 9, '\\x93': 100, 'e': 67, '2': 19, '\\x98': 104, '\\x88': 92, '[': 58, 'a': 63, 'L': 43, '\\x9e': 108, 'ã': 118, 'Y': 56, '¤': 112, 'd': 66, ')': 10, '.': 15, ',': 13, '?': 31, '\\x99': 105, '\\n': 0, '¡': 110, '9': 26, 'Q': 48, '^': 61, 'P': 47, '8': 25}\n",
      "int_to_vocab {0: '\\n', 1: '\\x0c', 2: ' ', 3: '!', 4: '\"', 5: '#', 6: '%', 7: '&', 8: \"'\", 9: '(', 10: ')', 11: '*', 12: '+', 13: ',', 14: '-', 15: '.', 16: '/', 17: '0', 18: '1', 19: '2', 20: '3', 21: '4', 22: '5', 23: '6', 24: '7', 25: '8', 26: '9', 27: ':', 28: ';', 29: '<', 30: '>', 31: '?', 32: 'A', 33: 'B', 34: 'C', 35: 'D', 36: 'E', 37: 'F', 38: 'G', 39: 'H', 40: 'I', 41: 'J', 42: 'K', 43: 'L', 44: 'M', 45: 'N', 46: 'O', 47: 'P', 48: 'Q', 49: 'R', 50: 'S', 51: 'T', 52: 'U', 53: 'V', 54: 'W', 55: 'X', 56: 'Y', 57: 'Z', 58: '[', 59: '\\\\', 60: ']', 61: '^', 62: '_', 63: 'a', 64: 'b', 65: 'c', 66: 'd', 67: 'e', 68: 'f', 69: 'g', 70: 'h', 71: 'i', 72: 'j', 73: 'k', 74: 'l', 75: 'm', 76: 'n', 77: 'o', 78: 'p', 79: 'q', 80: 'r', 81: 's', 82: 't', 83: 'u', 84: 'v', 85: 'w', 86: 'x', 87: 'y', 88: 'z', 89: '~', 90: '\\x82', 91: '\\x86', 92: '\\x88', 93: '\\x89', 94: '\\x8a', 95: '\\x8d', 96: '\\x8e', 97: '\\x8f', 98: '\\x90', 99: '\\x91', 100: '\\x93', 101: '\\x94', 102: '\\x95', 103: '\\x97', 104: '\\x98', 105: '\\x99', 106: '\\x9a', 107: '\\x9d', 108: '\\x9e', 109: '\\x9f', 110: '¡', 111: '£', 112: '¤', 113: '¥', 114: '§', 115: 'Ç', 116: 'È', 117: 'Ñ', 118: 'ã', 119: 'å', 120: 'æ', 121: 'è', 122: 'ë', 123: 'ó'}\n"
     ]
    }
   ],
   "source": [
    "print('vocab', vocab)\n",
    "print('vocab_to_int', vocab_to_int)\n",
    "print('int_to_vocab', int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISMAIL \\nKADARE \\n\\nGJAKFTOHTèSIA \\n\\nNOVELA \\n\\nSHTèPIA BOTUESE ÇNAIM FRASHèRIÈ \\n\\n\\x0c\\nRedaktor \\n\\n\\x0c\\ns \\n\\nSilva'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Characters encoded as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 40,  50,  44,  32,  40,  43,   2,   0,  42,  32,  35,  32,  49,\n",
       "        36,   2,   0,   0,  38,  41,  32,  42,  37,  51,  46,  39,  51,\n",
       "       121,  50,  40,  32,   2,   0,   0,  45,  46,  53,  36,  43,  32,\n",
       "         2,   0,   0,  50,  39,  51, 121,  47,  40,  32,   2,  33,  46,\n",
       "        51,  52,  36,  50,  36,   2, 115,  45,  32,  40,  44,   2,  37,\n",
       "        49,  32,  50,  39, 121,  49,  40, 116,   2,   0,   0,   1,   0,\n",
       "        49,  67,  66,  63,  73,  82,  77,  80,   2,   0,   0,   1,   0,\n",
       "        81,   2,   0,   0,  50,  71,  74,  84,  63], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many character classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = batch_size * n_steps\n",
    "    n_batches = len(arr) // characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches*characters_per_batch]\n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape(batch_size,-1)\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1] = x[:, 1:]\n",
    "        y[:, -1] = x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test get_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[40 50 44 32 40 43  2  0 42 32]\n",
      " [ 0 82 99  2 81 63 72 13  2 73]\n",
      " [ 2 78 83 81 83 74 74 99 15  2]\n",
      " [78 80 67 72  2 81 67 65 71 74]\n",
      " [72 77 80 69 83  2 78 99 80 78]\n",
      " [84 67 13  2 79 99  2 66 83 73]\n",
      " [64 83 81 70 83 80  2 75 67  2]\n",
      " [63  2 71 81 70 82 67  2 78 77]\n",
      " [77 71  2 78 99 80 78 63 80 63]\n",
      " [99 80 13  2 79 99  2 71 81 70]]\n",
      "\n",
      "y\n",
      " [[50 44 32 40 43  2  0 42 32 35]\n",
      " [82 99  2 81 63 72 13  2 73 63]\n",
      " [78 83 81 83 74 74 99 15  2 50]\n",
      " [80 67 72  2 81 67 65 71 74 71]\n",
      " [77 80 69 83  2 78 99 80 78 71]\n",
      " [67 13  2 79 99  2 66 83 73 67]\n",
      " [83 81 70 83 80  2 75 67  2 81]\n",
      " [ 2 71 81 70 82 67  2 78 77 88]\n",
      " [71  2 78 99 80 78 63 80 63  2]\n",
      " [80 13  2 79 99  2 71 81 70 82]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    def build_cell(num_units, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "        # Add dropout to the cell outputs\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped =  tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes) \n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.5          # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50...  Training Step: 50...  Training loss: 3.2348...  0.3226 sec/batch\n",
      "Epoch: 2/50...  Training Step: 100...  Training loss: 3.1186...  0.3251 sec/batch\n",
      "Epoch: 2/50...  Training Step: 150...  Training loss: 2.9233...  0.3225 sec/batch\n",
      "Epoch: 3/50...  Training Step: 200...  Training loss: 2.5858...  0.3219 sec/batch\n",
      "Epoch: 3/50...  Training Step: 250...  Training loss: 2.4124...  0.3234 sec/batch\n",
      "Epoch: 4/50...  Training Step: 300...  Training loss: 2.3354...  0.3225 sec/batch\n",
      "Epoch: 4/50...  Training Step: 350...  Training loss: 2.2782...  0.3249 sec/batch\n",
      "Epoch: 5/50...  Training Step: 400...  Training loss: 2.2512...  0.3265 sec/batch\n",
      "Epoch: 6/50...  Training Step: 450...  Training loss: 2.2019...  0.3253 sec/batch\n",
      "Epoch: 6/50...  Training Step: 500...  Training loss: 2.1474...  0.3213 sec/batch\n",
      "Epoch: 7/50...  Training Step: 550...  Training loss: 2.1120...  0.3244 sec/batch\n",
      "Epoch: 7/50...  Training Step: 600...  Training loss: 2.0558...  0.3256 sec/batch\n",
      "Epoch: 8/50...  Training Step: 650...  Training loss: 2.0230...  0.3221 sec/batch\n",
      "Epoch: 8/50...  Training Step: 700...  Training loss: 1.9534...  0.3267 sec/batch\n",
      "Epoch: 9/50...  Training Step: 750...  Training loss: 1.9617...  0.3249 sec/batch\n",
      "Epoch: 9/50...  Training Step: 800...  Training loss: 1.8782...  0.3262 sec/batch\n",
      "Epoch: 10/50...  Training Step: 850...  Training loss: 1.8537...  0.3245 sec/batch\n",
      "Epoch: 11/50...  Training Step: 900...  Training loss: 1.8273...  0.3222 sec/batch\n",
      "Epoch: 11/50...  Training Step: 950...  Training loss: 1.8160...  0.3268 sec/batch\n",
      "Epoch: 12/50...  Training Step: 1000...  Training loss: 1.7574...  0.3221 sec/batch\n",
      "Epoch: 12/50...  Training Step: 1050...  Training loss: 1.7514...  0.3245 sec/batch\n",
      "Epoch: 13/50...  Training Step: 1100...  Training loss: 1.7102...  0.3257 sec/batch\n",
      "Epoch: 13/50...  Training Step: 1150...  Training loss: 1.6848...  0.3251 sec/batch\n",
      "Epoch: 14/50...  Training Step: 1200...  Training loss: 1.6692...  0.3229 sec/batch\n",
      "Epoch: 15/50...  Training Step: 1250...  Training loss: 1.6606...  0.3216 sec/batch\n",
      "Epoch: 15/50...  Training Step: 1300...  Training loss: 1.6203...  0.3255 sec/batch\n",
      "Epoch: 16/50...  Training Step: 1350...  Training loss: 1.6232...  0.3229 sec/batch\n",
      "Epoch: 16/50...  Training Step: 1400...  Training loss: 1.5936...  0.3269 sec/batch\n",
      "Epoch: 17/50...  Training Step: 1450...  Training loss: 1.6242...  0.3262 sec/batch\n",
      "Epoch: 17/50...  Training Step: 1500...  Training loss: 1.5633...  0.3211 sec/batch\n",
      "Epoch: 18/50...  Training Step: 1550...  Training loss: 1.5721...  0.3219 sec/batch\n",
      "Epoch: 18/50...  Training Step: 1600...  Training loss: 1.5231...  0.3241 sec/batch\n",
      "Epoch: 19/50...  Training Step: 1650...  Training loss: 1.5703...  0.3251 sec/batch\n",
      "Epoch: 20/50...  Training Step: 1700...  Training loss: 1.5455...  0.3268 sec/batch\n",
      "Epoch: 20/50...  Training Step: 1750...  Training loss: 1.4880...  0.3229 sec/batch\n",
      "Epoch: 21/50...  Training Step: 1800...  Training loss: 1.5088...  0.3246 sec/batch\n",
      "Epoch: 21/50...  Training Step: 1850...  Training loss: 1.5025...  0.3264 sec/batch\n",
      "Epoch: 22/50...  Training Step: 1900...  Training loss: 1.4776...  0.3259 sec/batch\n",
      "Epoch: 22/50...  Training Step: 1950...  Training loss: 1.4844...  0.3271 sec/batch\n",
      "Epoch: 23/50...  Training Step: 2000...  Training loss: 1.4699...  0.3222 sec/batch\n",
      "Epoch: 24/50...  Training Step: 2050...  Training loss: 1.4716...  0.3273 sec/batch\n",
      "Epoch: 24/50...  Training Step: 2100...  Training loss: 1.4565...  0.3266 sec/batch\n",
      "Epoch: 25/50...  Training Step: 2150...  Training loss: 1.3975...  0.3238 sec/batch\n",
      "Epoch: 25/50...  Training Step: 2200...  Training loss: 1.4505...  0.3260 sec/batch\n",
      "Epoch: 26/50...  Training Step: 2250...  Training loss: 1.4434...  0.3264 sec/batch\n",
      "Epoch: 26/50...  Training Step: 2300...  Training loss: 1.4017...  0.3282 sec/batch\n",
      "Epoch: 27/50...  Training Step: 2350...  Training loss: 1.4447...  0.3234 sec/batch\n",
      "Epoch: 27/50...  Training Step: 2400...  Training loss: 1.3990...  0.3262 sec/batch\n",
      "Epoch: 28/50...  Training Step: 2450...  Training loss: 1.4019...  0.3279 sec/batch\n",
      "Epoch: 29/50...  Training Step: 2500...  Training loss: 1.4234...  0.3246 sec/batch\n",
      "Epoch: 29/50...  Training Step: 2550...  Training loss: 1.3671...  0.3253 sec/batch\n",
      "Epoch: 30/50...  Training Step: 2600...  Training loss: 1.3717...  0.3250 sec/batch\n",
      "Epoch: 30/50...  Training Step: 2650...  Training loss: 1.3486...  0.3254 sec/batch\n",
      "Epoch: 31/50...  Training Step: 2700...  Training loss: 1.3809...  0.3271 sec/batch\n",
      "Epoch: 31/50...  Training Step: 2750...  Training loss: 1.3766...  0.3250 sec/batch\n",
      "Epoch: 32/50...  Training Step: 2800...  Training loss: 1.3565...  0.3241 sec/batch\n",
      "Epoch: 33/50...  Training Step: 2850...  Training loss: 1.3183...  0.3229 sec/batch\n",
      "Epoch: 33/50...  Training Step: 2900...  Training loss: 1.3547...  0.3254 sec/batch\n",
      "Epoch: 34/50...  Training Step: 2950...  Training loss: 1.3226...  0.3270 sec/batch\n",
      "Epoch: 34/50...  Training Step: 3000...  Training loss: 1.3823...  0.3237 sec/batch\n",
      "Epoch: 35/50...  Training Step: 3050...  Training loss: 1.3176...  0.3281 sec/batch\n",
      "Epoch: 35/50...  Training Step: 3100...  Training loss: 1.3177...  0.3276 sec/batch\n",
      "Epoch: 36/50...  Training Step: 3150...  Training loss: 1.2980...  0.3272 sec/batch\n",
      "Epoch: 36/50...  Training Step: 3200...  Training loss: 1.3235...  0.3271 sec/batch\n",
      "Epoch: 37/50...  Training Step: 3250...  Training loss: 1.2819...  0.3251 sec/batch\n",
      "Epoch: 38/50...  Training Step: 3300...  Training loss: 1.3128...  0.3244 sec/batch\n",
      "Epoch: 38/50...  Training Step: 3350...  Training loss: 1.3283...  0.3268 sec/batch\n",
      "Epoch: 39/50...  Training Step: 3400...  Training loss: 1.3019...  0.3266 sec/batch\n",
      "Epoch: 39/50...  Training Step: 3450...  Training loss: 1.2905...  0.3229 sec/batch\n",
      "Epoch: 40/50...  Training Step: 3500...  Training loss: 1.2815...  0.3217 sec/batch\n",
      "Epoch: 40/50...  Training Step: 3550...  Training loss: 1.3056...  0.3215 sec/batch\n",
      "Epoch: 41/50...  Training Step: 3600...  Training loss: 1.2559...  0.3243 sec/batch\n",
      "Epoch: 42/50...  Training Step: 3650...  Training loss: 1.3441...  0.3262 sec/batch\n",
      "Epoch: 42/50...  Training Step: 3700...  Training loss: 1.2328...  0.3220 sec/batch\n",
      "Epoch: 43/50...  Training Step: 3750...  Training loss: 1.2480...  0.3213 sec/batch\n",
      "Epoch: 43/50...  Training Step: 3800...  Training loss: 1.2650...  0.3256 sec/batch\n",
      "Epoch: 44/50...  Training Step: 3850...  Training loss: 1.2516...  0.3231 sec/batch\n",
      "Epoch: 44/50...  Training Step: 3900...  Training loss: 1.2320...  0.3219 sec/batch\n",
      "Epoch: 45/50...  Training Step: 3950...  Training loss: 1.2092...  0.3271 sec/batch\n",
      "Epoch: 45/50...  Training Step: 4000...  Training loss: 1.2451...  0.3265 sec/batch\n",
      "Epoch: 46/50...  Training Step: 4050...  Training loss: 1.2206...  0.3253 sec/batch\n",
      "Epoch: 47/50...  Training Step: 4100...  Training loss: 1.2395...  0.3229 sec/batch\n",
      "Epoch: 47/50...  Training Step: 4150...  Training loss: 1.2373...  0.3222 sec/batch\n",
      "Epoch: 48/50...  Training Step: 4200...  Training loss: 1.2343...  0.3233 sec/batch\n",
      "Epoch: 48/50...  Training Step: 4250...  Training loss: 1.2243...  0.3238 sec/batch\n",
      "Epoch: 49/50...  Training Step: 4300...  Training loss: 1.1995...  0.3252 sec/batch\n",
      "Epoch: 49/50...  Training Step: 4350...  Training loss: 1.1944...  0.3247 sec/batch\n",
      "Epoch: 50/50...  Training Step: 4400...  Training loss: 1.1961...  0.3275 sec/batch\n",
      "Epoch: 50/50...  Training Step: 4450...  Training loss: 1.1866...  0.3249 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "# Print losses every N interations\n",
    "print_every_n = 50\n",
    "\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i4450_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4450_l512.ckpt\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i4450_l512.ckpt'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ai marrdhnies \n",
      "prsri pr t gjetur syrin, nuk ishte ajo \n",
      "q n kulln e korredorives. N kmb t gjata e \n",
      "\n",
      "\f",
      "\n",
      "kuptonte sekse n kt are dhe t madheshim, kurse \n",
      "me sgjetr n fil,, pa i stiu q ajo thoshie: si t pari.. \n",
      "\n",
      "Ñ Kur u dyshohm me kam dol q nuk e dini.., \n",
      "Ñ vrtiteti njerzit pa e shpruar prjashta persazluar. \n",
      "Ñ Diku n sa m mire, Ñ tha ai. Ñ Ai do t \n",
      "ista prej tij. \n",
      "Ñ Njzna kto gate, Ñ na t'i bejonte nj ndezi \n",
      "njerbi. Ai vshtroi nga nisur prap. \n",
      "ÑE pakori. \n",
      "Ñ S'sht at nga ktu manjohe, pikrisht kndeje. \n",
      "Ñ Kursh dhe se si ishte trengaulliri i madh dhe \n",
      "errit ndonj gjq, madje kur ndeshi nj gjum pare \n",
      "e ngrisin me vali. Ajo i kalojen pak.Èa \n",
      "t prgjigjur se ndoshta kush do t iku me sy. \n",
      "Ñ Kur shkrimtari thoshin shum m t zrunshen t \n",
      "prqafuar syt me dashrore. \n",
      "Ali Binaku e pa se miran nj cop her t zymt \n",
      "t mdhenj pr t kthyer kto. \n",
      "\n",
      "Ñ Krishm. \n",
      "Ñ Ngunjlern e varreve me t shoqet, prkundrazi, \n",
      "quajts si, ndoshta ndodhej n katund pm sdreqin, \n",
      "e njohte me shpejtsi. \n",
      "Nn korridor doli prjush tem doli. \n",
      "Ñ Pr t mundsi sa ndryshome tavonin ploshmi, \n",
      "Ñ tha paa e m siguri, Ñ shku shkak pr nj \n",
      "nua t mbrrinn pr t'u mbshtetur n ndorjellin e \n",
      "trenit jan t njerz dhe nga ndonj nga steci, nga \n",
      "nj majts. \n",
      "\n",
      "ÑShoku Fallko, Ñ tha ajo, pr shum te shtrgnata \n",
      "e shkaqet, nga trupi t posht, sepse kjo nuk ia kishte \n",
      "pasur, por, ngaq u duk me se ia dish t'ju prgjigur asnjeri. \n",
      "Ñ Shum got e dshmondet prapa tyre? Ñ tha Stresi me \n",
      "vese shpesh pas pastaj s kanunin, s'po shkej t \n",
      "ndodhte, por nj gj tjetr her punt,dita e tij, m \n",
      "sipuron, n qiell, n t vshtir t teperohej n njerzi. \n",
      "\n",
      "ÑPo, Ñ tha duke vshtruar pr ajo ditroi desha t shum\n",
      "t prej tyre. Ñ Ti e pra, n fark pakur prejtimet e sikurt \n",
      "q kishte prcin seke. \n",
      "Ata nuk i donte ta kte njrin pr faqet e tij. \n",
      "\n",
      "\f",
      "\n",
      "IPsa katr nent e tyre ishin t vmansure, shprehen \n",
      "nns pr t pritur n mest. \n",
      "\n",
      "ÑPo si. \n",
      "Ajo ngriti kokn, ngaia e sa mbante.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Ai\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
