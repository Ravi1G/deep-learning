{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ismail Kadare - Gjakftohtesia\n",
    "\n",
    "This notebook builds a character-wise RNN trained on \"Gjakftohtesia\" of the albanian author Ismail Kadare. It'll be used to generate a new chapter.\n",
    "\n",
    "This network is based on https://github.com/udacity/deep-learning/tree/master/intro-to-rnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Ismail Kadare - Gjakftohtesia.txt', 'r', encoding=\"iso-8859-1\") as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab ['\\n', '\\x0c', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '\\x82', '\\x86', '\\x88', '\\x89', '\\x8a', '\\x8d', '\\x8e', '\\x8f', '\\x90', '\\x91', '\\x93', '\\x94', '\\x95', '\\x97', '\\x98', '\\x99', '\\x9a', '\\x9d', '\\x9e', '\\x9f', '¡', '£', '¤', '¥', '§', 'Ç', 'È', 'Ñ', 'ã', 'å', 'æ', 'è', 'ë', 'ó']\n",
      "vocab_to_int {'\\x8e': 96, 'L': 43, 'l': 74, 'D': 35, '+': 12, 'å': 119, 'F': 37, 'V': 53, 'Q': 48, 'b': 64, '&': 7, 'R': 49, '\"': 4, '?': 31, ',': 13, '\\n': 0, 'è': 121, ':': 27, '9': 26, ')': 10, '0': 17, '\\x8f': 97, 'e': 67, '\\\\': 59, 'f': 68, '\\x86': 91, 'C': 34, 'Z': 57, '/': 16, '(': 9, '!': 3, '\\x8d': 95, '1': 18, 's': 81, '7': 24, '\\x0c': 1, 'a': 63, '\\x99': 105, 'r': 80, 'm': 75, \"'\": 8, 'c': 65, '\\x9e': 108, '5': 22, '\\x95': 102, '\\x93': 100, 'G': 38, 'È': 116, '¥': 113, 'X': 55, 'K': 42, 'O': 46, 'I': 40, 'S': 50, 'P': 47, ';': 28, 'u': 83, 'g': 69, '\\x82': 90, '¤': 112, 'W': 54, 'Ñ': 117, 'M': 44, 'h': 70, '\\x8a': 94, '%': 6, 'v': 84, 'T': 51, 'ã': 118, 'U': 52, '§': 114, '.': 15, 'q': 79, 'i': 71, '*': 11, '\\x88': 92, 'B': 33, '£': 111, '\\x90': 98, 'x': 86, '\\x91': 99, 'k': 73, '#': 5, '¡': 110, '6': 23, '\\x97': 103, 't': 82, 'p': 78, 'Y': 56, 'ó': 123, '\\x94': 101, 'j': 72, 'y': 87, '~': 89, 'ë': 122, 'Ç': 115, 'J': 41, '[': 58, 'd': 66, '3': 20, 'A': 32, 'E': 36, '^': 61, 'N': 45, '\\x9a': 106, '\\x9d': 107, ']': 60, '4': 21, 'z': 88, '_': 62, 'o': 77, '\\x98': 104, 'æ': 120, 'H': 39, 'w': 85, '<': 29, ' ': 2, '-': 14, '2': 19, '\\x89': 93, 'n': 76, '\\x9f': 109, '>': 30, '8': 25}\n",
      "int_to_vocab {0: '\\n', 1: '\\x0c', 2: ' ', 3: '!', 4: '\"', 5: '#', 6: '%', 7: '&', 8: \"'\", 9: '(', 10: ')', 11: '*', 12: '+', 13: ',', 14: '-', 15: '.', 16: '/', 17: '0', 18: '1', 19: '2', 20: '3', 21: '4', 22: '5', 23: '6', 24: '7', 25: '8', 26: '9', 27: ':', 28: ';', 29: '<', 30: '>', 31: '?', 32: 'A', 33: 'B', 34: 'C', 35: 'D', 36: 'E', 37: 'F', 38: 'G', 39: 'H', 40: 'I', 41: 'J', 42: 'K', 43: 'L', 44: 'M', 45: 'N', 46: 'O', 47: 'P', 48: 'Q', 49: 'R', 50: 'S', 51: 'T', 52: 'U', 53: 'V', 54: 'W', 55: 'X', 56: 'Y', 57: 'Z', 58: '[', 59: '\\\\', 60: ']', 61: '^', 62: '_', 63: 'a', 64: 'b', 65: 'c', 66: 'd', 67: 'e', 68: 'f', 69: 'g', 70: 'h', 71: 'i', 72: 'j', 73: 'k', 74: 'l', 75: 'm', 76: 'n', 77: 'o', 78: 'p', 79: 'q', 80: 'r', 81: 's', 82: 't', 83: 'u', 84: 'v', 85: 'w', 86: 'x', 87: 'y', 88: 'z', 89: '~', 90: '\\x82', 91: '\\x86', 92: '\\x88', 93: '\\x89', 94: '\\x8a', 95: '\\x8d', 96: '\\x8e', 97: '\\x8f', 98: '\\x90', 99: '\\x91', 100: '\\x93', 101: '\\x94', 102: '\\x95', 103: '\\x97', 104: '\\x98', 105: '\\x99', 106: '\\x9a', 107: '\\x9d', 108: '\\x9e', 109: '\\x9f', 110: '¡', 111: '£', 112: '¤', 113: '¥', 114: '§', 115: 'Ç', 116: 'È', 117: 'Ñ', 118: 'ã', 119: 'å', 120: 'æ', 121: 'è', 122: 'ë', 123: 'ó'}\n"
     ]
    }
   ],
   "source": [
    "print('vocab', vocab)\n",
    "print('vocab_to_int', vocab_to_int)\n",
    "print('int_to_vocab', int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISMAIL \\nKADARE \\n\\nGJAKFTOHTèSIA \\n\\nNOVELA \\n\\nSHTèPIA BOTUESE ÇNAIM FRASHèRIÈ \\n\\n\\x0c\\nRedaktor \\n\\n\\x0c\\ns \\n\\nSilva'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Characters encoded as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 40,  50,  44,  32,  40,  43,   2,   0,  42,  32,  35,  32,  49,\n",
       "        36,   2,   0,   0,  38,  41,  32,  42,  37,  51,  46,  39,  51,\n",
       "       121,  50,  40,  32,   2,   0,   0,  45,  46,  53,  36,  43,  32,\n",
       "         2,   0,   0,  50,  39,  51, 121,  47,  40,  32,   2,  33,  46,\n",
       "        51,  52,  36,  50,  36,   2, 115,  45,  32,  40,  44,   2,  37,\n",
       "        49,  32,  50,  39, 121,  49,  40, 116,   2,   0,   0,   1,   0,\n",
       "        49,  67,  66,  63,  73,  82,  77,  80,   2,   0,   0,   1,   0,\n",
       "        81,   2,   0,   0,  50,  71,  74,  84,  63], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many character classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = batch_size * n_steps\n",
    "    n_batches = len(arr) // characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches*characters_per_batch]\n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape(batch_size,-1)\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1] = x[:, 1:]\n",
    "        y[:, -1] = x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test get_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[40 50 44 32 40 43  2  0 42 32]\n",
      " [ 0 82 99  2 81 63 72 13  2 73]\n",
      " [ 2 78 83 81 83 74 74 99 15  2]\n",
      " [78 80 67 72  2 81 67 65 71 74]\n",
      " [72 77 80 69 83  2 78 99 80 78]\n",
      " [84 67 13  2 79 99  2 66 83 73]\n",
      " [64 83 81 70 83 80  2 75 67  2]\n",
      " [63  2 71 81 70 82 67  2 78 77]\n",
      " [77 71  2 78 99 80 78 63 80 63]\n",
      " [99 80 13  2 79 99  2 71 81 70]]\n",
      "\n",
      "y\n",
      " [[50 44 32 40 43  2  0 42 32 35]\n",
      " [82 99  2 81 63 72 13  2 73 63]\n",
      " [78 83 81 83 74 74 99 15  2 50]\n",
      " [80 67 72  2 81 67 65 71 74 71]\n",
      " [77 80 69 83  2 78 99 80 78 71]\n",
      " [67 13  2 79 99  2 66 83 73 67]\n",
      " [83 81 70 83 80  2 75 67  2 81]\n",
      " [ 2 71 81 70 82 67  2 78 77 88]\n",
      " [71  2 78 99 80 78 63 80 63  2]\n",
      " [80 13  2 79 99  2 71 81 70 82]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    def build_cell(num_units, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "        # Add dropout to the cell outputs\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped =  tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes) \n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10         # Sequences per batch\n",
    "num_steps = 50          # Number of sequence steps per batch\n",
    "lstm_size = 128         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.01    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 50...  Training loss: 2.9993...  0.0554 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 2.7332...  0.0551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.6128...  0.0558 sec/batch\n",
      "Epoch: 1/20...  Training Step: 200...  Training loss: 2.4931...  0.0574 sec/batch\n",
      "Epoch: 1/20...  Training Step: 250...  Training loss: 2.4146...  0.0544 sec/batch\n",
      "Epoch: 1/20...  Training Step: 300...  Training loss: 2.3376...  0.0543 sec/batch\n",
      "Epoch: 1/20...  Training Step: 350...  Training loss: 2.3415...  0.0576 sec/batch\n",
      "Epoch: 1/20...  Training Step: 400...  Training loss: 2.3921...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 450...  Training loss: 2.1877...  0.0532 sec/batch\n",
      "Epoch: 1/20...  Training Step: 500...  Training loss: 2.1777...  0.0540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 550...  Training loss: 2.2191...  0.0597 sec/batch\n",
      "Epoch: 1/20...  Training Step: 600...  Training loss: 2.1889...  0.0566 sec/batch\n",
      "Epoch: 1/20...  Training Step: 650...  Training loss: 2.2586...  0.0541 sec/batch\n",
      "Epoch: 1/20...  Training Step: 700...  Training loss: 2.1592...  0.0600 sec/batch\n",
      "Epoch: 1/20...  Training Step: 750...  Training loss: 2.1213...  0.0582 sec/batch\n",
      "Epoch: 1/20...  Training Step: 800...  Training loss: 2.1089...  0.0600 sec/batch\n",
      "Epoch: 1/20...  Training Step: 850...  Training loss: 2.2045...  0.0537 sec/batch\n",
      "Epoch: 1/20...  Training Step: 900...  Training loss: 2.2516...  0.0538 sec/batch\n",
      "Epoch: 1/20...  Training Step: 950...  Training loss: 2.1514...  0.0576 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1000...  Training loss: 2.0603...  0.0590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1050...  Training loss: 2.1925...  0.0579 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1100...  Training loss: 2.1206...  0.0552 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1150...  Training loss: 2.0072...  0.0583 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1200...  Training loss: 2.1303...  0.0590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1250...  Training loss: 2.2131...  0.0590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1300...  Training loss: 1.9860...  0.0560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1350...  Training loss: 2.0394...  0.0541 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1400...  Training loss: 2.1321...  0.0578 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1450...  Training loss: 2.0518...  0.0542 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1500...  Training loss: 2.0841...  0.0581 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1550...  Training loss: 1.8660...  0.0573 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1600...  Training loss: 1.8926...  0.0546 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1650...  Training loss: 1.9600...  0.0572 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1700...  Training loss: 1.9008...  0.0583 sec/batch\n",
      "Epoch: 1/20...  Training Step: 1750...  Training loss: 1.9652...  0.0538 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1800...  Training loss: 1.9600...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1850...  Training loss: 2.0194...  0.0601 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1900...  Training loss: 1.8243...  0.0581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 1950...  Training loss: 1.9800...  0.0587 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2000...  Training loss: 2.1546...  0.0602 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2050...  Training loss: 1.9298...  0.0587 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2100...  Training loss: 1.8351...  0.0564 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2150...  Training loss: 2.0062...  0.0600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2200...  Training loss: 1.8783...  0.0559 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2250...  Training loss: 2.0117...  0.0569 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2300...  Training loss: 2.0544...  0.0535 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2350...  Training loss: 2.0247...  0.0547 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2400...  Training loss: 1.9126...  0.0598 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2450...  Training loss: 1.9031...  0.0586 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2500...  Training loss: 2.1181...  0.0557 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2550...  Training loss: 1.9518...  0.0541 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2600...  Training loss: 1.9495...  0.0595 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2650...  Training loss: 1.9547...  0.0578 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2700...  Training loss: 2.0760...  0.0580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2750...  Training loss: 1.8843...  0.0578 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2800...  Training loss: 2.0284...  0.0573 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2850...  Training loss: 1.9303...  0.0554 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2900...  Training loss: 1.8858...  0.0532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 2950...  Training loss: 2.0891...  0.0558 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3000...  Training loss: 1.8943...  0.0567 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3050...  Training loss: 1.8010...  0.0553 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3100...  Training loss: 2.0248...  0.0533 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3150...  Training loss: 1.9635...  0.0574 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3200...  Training loss: 1.9963...  0.0571 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3250...  Training loss: 1.9869...  0.0597 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3300...  Training loss: 2.0495...  0.0601 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3350...  Training loss: 1.9095...  0.0526 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3400...  Training loss: 1.9460...  0.0566 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3450...  Training loss: 1.8949...  0.0593 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3500...  Training loss: 1.8043...  0.0542 sec/batch\n",
      "Epoch: 2/20...  Training Step: 3550...  Training loss: 1.8745...  0.0599 sec/batch\n",
      "Epoch: 3/20...  Training Step: 3600...  Training loss: 1.9071...  0.0591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 3650...  Training loss: 1.8100...  0.0527 sec/batch\n",
      "Epoch: 3/20...  Training Step: 3700...  Training loss: 2.0294...  0.0574 sec/batch\n",
      "Epoch: 3/20...  Training Step: 3750...  Training loss: 1.9478...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 3800...  Training loss: 1.9042...  0.0543 sec/batch\n",
      "Epoch: 3/20...  Training Step: 3850...  Training loss: 1.8781...  0.0540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 3900...  Training loss: 1.8440...  0.0556 sec/batch\n",
      "Epoch: 3/20...  Training Step: 3950...  Training loss: 1.9895...  0.0537 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4000...  Training loss: 2.0013...  0.0541 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4050...  Training loss: 1.9301...  0.0540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4100...  Training loss: 1.9074...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4150...  Training loss: 1.9754...  0.0548 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4200...  Training loss: 1.9898...  0.0584 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4250...  Training loss: 1.9104...  0.0577 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4300...  Training loss: 1.8362...  0.0595 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4350...  Training loss: 1.8959...  0.0548 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4400...  Training loss: 1.9533...  0.0531 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4450...  Training loss: 2.0367...  0.0592 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4500...  Training loss: 1.9243...  0.0583 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4550...  Training loss: 1.8122...  0.0534 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4600...  Training loss: 1.8808...  0.0588 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4650...  Training loss: 1.9262...  0.0538 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4700...  Training loss: 1.9404...  0.0532 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4750...  Training loss: 1.8896...  0.0566 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4800...  Training loss: 1.8453...  0.0599 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4850...  Training loss: 1.9284...  0.0546 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4900...  Training loss: 1.9127...  0.0594 sec/batch\n",
      "Epoch: 3/20...  Training Step: 4950...  Training loss: 1.8290...  0.0576 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 5000...  Training loss: 1.8790...  0.0544 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5050...  Training loss: 1.9629...  0.0573 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5100...  Training loss: 1.9858...  0.0539 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5150...  Training loss: 1.9960...  0.0554 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5200...  Training loss: 1.7993...  0.0535 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5250...  Training loss: 1.9475...  0.0571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5300...  Training loss: 1.8610...  0.0544 sec/batch\n",
      "Epoch: 3/20...  Training Step: 5350...  Training loss: 1.9023...  0.0538 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5400...  Training loss: 1.9034...  0.0580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5450...  Training loss: 1.9979...  0.0580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5500...  Training loss: 1.7879...  0.0532 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5550...  Training loss: 1.8614...  0.0568 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5600...  Training loss: 1.8589...  0.0539 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5650...  Training loss: 1.8099...  0.0581 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5700...  Training loss: 1.9682...  0.0563 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5750...  Training loss: 1.8898...  0.0533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5800...  Training loss: 1.8315...  0.0600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5850...  Training loss: 1.7600...  0.0570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5900...  Training loss: 1.7170...  0.0578 sec/batch\n",
      "Epoch: 4/20...  Training Step: 5950...  Training loss: 1.7366...  0.0535 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6000...  Training loss: 1.6308...  0.0558 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6050...  Training loss: 1.7983...  0.0531 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6100...  Training loss: 2.0003...  0.0579 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6150...  Training loss: 1.8300...  0.0542 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6200...  Training loss: 1.9681...  0.0535 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6250...  Training loss: 1.9811...  0.0539 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6300...  Training loss: 1.9370...  0.0572 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6350...  Training loss: 1.8652...  0.0593 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6400...  Training loss: 1.9356...  0.0571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6450...  Training loss: 1.9139...  0.0537 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6500...  Training loss: 1.9189...  0.0527 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6550...  Training loss: 1.9603...  0.0524 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6600...  Training loss: 1.8407...  0.0566 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6650...  Training loss: 1.8400...  0.0571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6700...  Training loss: 1.9081...  0.0529 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6750...  Training loss: 1.7868...  0.0525 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6800...  Training loss: 1.7784...  0.0586 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6850...  Training loss: 1.7124...  0.0553 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6900...  Training loss: 1.8737...  0.0556 sec/batch\n",
      "Epoch: 4/20...  Training Step: 6950...  Training loss: 1.8822...  0.0576 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7000...  Training loss: 1.9124...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7050...  Training loss: 1.9724...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7100...  Training loss: 1.7787...  0.0542 sec/batch\n",
      "Epoch: 4/20...  Training Step: 7150...  Training loss: 1.8629...  0.0598 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7200...  Training loss: 1.7696...  0.0536 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7250...  Training loss: 1.8482...  0.0549 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7300...  Training loss: 1.8514...  0.0552 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7350...  Training loss: 1.8193...  0.0540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7400...  Training loss: 1.8469...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7450...  Training loss: 1.5518...  0.0572 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7500...  Training loss: 1.8592...  0.0533 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7550...  Training loss: 1.7298...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7600...  Training loss: 2.0078...  0.0554 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7650...  Training loss: 1.7344...  0.0532 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7700...  Training loss: 1.9003...  0.0553 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7750...  Training loss: 1.9876...  0.0577 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7800...  Training loss: 1.8767...  0.0535 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7850...  Training loss: 1.8235...  0.0558 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7900...  Training loss: 1.8801...  0.0569 sec/batch\n",
      "Epoch: 5/20...  Training Step: 7950...  Training loss: 1.8904...  0.0569 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8000...  Training loss: 1.9987...  0.0563 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8050...  Training loss: 1.9399...  0.0587 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8100...  Training loss: 1.9419...  0.0561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8150...  Training loss: 1.9123...  0.0551 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8200...  Training loss: 1.8735...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8250...  Training loss: 1.8101...  0.0574 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8300...  Training loss: 1.8771...  0.0590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8350...  Training loss: 1.9263...  0.0604 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8400...  Training loss: 1.7481...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8450...  Training loss: 1.8691...  0.0588 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8500...  Training loss: 1.9805...  0.0529 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8550...  Training loss: 1.8805...  0.0600 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8600...  Training loss: 1.9169...  0.0568 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8650...  Training loss: 1.8480...  0.0564 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8700...  Training loss: 1.9650...  0.0557 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8750...  Training loss: 1.9534...  0.0534 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8800...  Training loss: 1.8018...  0.0609 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8850...  Training loss: 1.7969...  0.0562 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8900...  Training loss: 1.8835...  0.0574 sec/batch\n",
      "Epoch: 5/20...  Training Step: 8950...  Training loss: 1.8108...  0.0581 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9000...  Training loss: 1.9114...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9050...  Training loss: 1.8337...  0.0554 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9100...  Training loss: 1.8495...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9150...  Training loss: 1.8440...  0.0537 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9200...  Training loss: 1.9538...  0.0604 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9250...  Training loss: 1.6919...  0.0547 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9300...  Training loss: 1.7855...  0.0601 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9350...  Training loss: 1.8754...  0.0558 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9400...  Training loss: 1.8655...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9450...  Training loss: 1.9401...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9500...  Training loss: 1.7742...  0.0521 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9550...  Training loss: 1.7900...  0.0553 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9600...  Training loss: 1.7781...  0.0532 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9650...  Training loss: 1.8531...  0.0551 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9700...  Training loss: 2.0188...  0.0593 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9750...  Training loss: 1.7061...  0.0548 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9800...  Training loss: 1.8260...  0.0568 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9850...  Training loss: 1.7363...  0.0526 sec/batch\n",
      "Epoch: 6/20...  Training Step: 9900...  Training loss: 1.8797...  0.0596 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 9950...  Training loss: 1.8407...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10000...  Training loss: 1.8873...  0.0561 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10050...  Training loss: 1.7573...  0.0583 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10100...  Training loss: 1.7461...  0.0588 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10150...  Training loss: 1.8376...  0.0541 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10200...  Training loss: 2.0065...  0.0580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10250...  Training loss: 1.8945...  0.0555 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10300...  Training loss: 1.8741...  0.0536 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10350...  Training loss: 1.7522...  0.0531 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10400...  Training loss: 1.8030...  0.0577 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10450...  Training loss: 1.9705...  0.0536 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10500...  Training loss: 1.7888...  0.0581 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10550...  Training loss: 1.8131...  0.0564 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10600...  Training loss: 1.7482...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10650...  Training loss: 1.7080...  0.0576 sec/batch\n",
      "Epoch: 6/20...  Training Step: 10700...  Training loss: 1.8059...  0.0585 sec/batch\n",
      "Epoch: 7/20...  Training Step: 10750...  Training loss: 1.7273...  0.0541 sec/batch\n",
      "Epoch: 7/20...  Training Step: 10800...  Training loss: 1.7264...  0.0527 sec/batch\n",
      "Epoch: 7/20...  Training Step: 10850...  Training loss: 1.8982...  0.0532 sec/batch\n",
      "Epoch: 7/20...  Training Step: 10900...  Training loss: 1.9168...  0.0597 sec/batch\n",
      "Epoch: 7/20...  Training Step: 10950...  Training loss: 1.9531...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11000...  Training loss: 1.8151...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11050...  Training loss: 1.9116...  0.0534 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11100...  Training loss: 1.9403...  0.0585 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11150...  Training loss: 1.7761...  0.0537 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11200...  Training loss: 1.7958...  0.0597 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11250...  Training loss: 1.7855...  0.0542 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11300...  Training loss: 1.9526...  0.0562 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11350...  Training loss: 1.9030...  0.0570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11400...  Training loss: 1.8528...  0.0561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11450...  Training loss: 1.8491...  0.0551 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11500...  Training loss: 1.8918...  0.0588 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11550...  Training loss: 1.8138...  0.0553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11600...  Training loss: 1.8812...  0.0536 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11650...  Training loss: 1.8744...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11700...  Training loss: 1.8094...  0.0594 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11750...  Training loss: 1.9174...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11800...  Training loss: 1.7748...  0.0545 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11850...  Training loss: 1.8171...  0.0563 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11900...  Training loss: 1.8253...  0.0579 sec/batch\n",
      "Epoch: 7/20...  Training Step: 11950...  Training loss: 1.8704...  0.0554 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12000...  Training loss: 1.9100...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12050...  Training loss: 1.8099...  0.0539 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12100...  Training loss: 1.9922...  0.0551 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12150...  Training loss: 1.8604...  0.0556 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12200...  Training loss: 1.8925...  0.0577 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12250...  Training loss: 1.7776...  0.0531 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12300...  Training loss: 1.8108...  0.0601 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12350...  Training loss: 1.7538...  0.0536 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12400...  Training loss: 1.8885...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12450...  Training loss: 1.7780...  0.0535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 12500...  Training loss: 1.9628...  0.0555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 12550...  Training loss: 1.7639...  0.0536 sec/batch\n",
      "Epoch: 8/20...  Training Step: 12600...  Training loss: 1.8868...  0.0560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 12650...  Training loss: 1.7790...  0.0543 sec/batch\n",
      "Epoch: 8/20...  Training Step: 12700...  Training loss: 1.8104...  0.0527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 12750...  Training loss: 1.8256...  0.0538 sec/batch\n",
      "Epoch: 8/20...  Training Step: 12800...  Training loss: 1.6190...  0.0572 sec/batch\n",
      "Epoch: 8/20...  Training Step: 12850...  Training loss: 1.7840...  0.0566 sec/batch\n",
      "Epoch: 8/20...  Training Step: 12900...  Training loss: 1.8174...  0.0559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 12950...  Training loss: 1.8620...  0.0537 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13000...  Training loss: 1.8550...  0.0557 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13050...  Training loss: 1.8055...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13100...  Training loss: 1.7834...  0.0533 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13150...  Training loss: 1.6756...  0.0568 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13200...  Training loss: 1.6438...  0.0598 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13250...  Training loss: 1.8206...  0.0601 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13300...  Training loss: 1.7814...  0.0547 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13350...  Training loss: 1.8331...  0.0556 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13400...  Training loss: 1.7053...  0.0552 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13450...  Training loss: 1.7705...  0.0548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13500...  Training loss: 1.8679...  0.0538 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13550...  Training loss: 1.7739...  0.0568 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13600...  Training loss: 1.8018...  0.0575 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13650...  Training loss: 1.7293...  0.0546 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13700...  Training loss: 1.8607...  0.0539 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13750...  Training loss: 1.7580...  0.0572 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13800...  Training loss: 1.7896...  0.0590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13850...  Training loss: 1.8265...  0.0578 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13900...  Training loss: 1.7893...  0.0574 sec/batch\n",
      "Epoch: 8/20...  Training Step: 13950...  Training loss: 1.7694...  0.0562 sec/batch\n",
      "Epoch: 8/20...  Training Step: 14000...  Training loss: 1.6332...  0.0538 sec/batch\n",
      "Epoch: 8/20...  Training Step: 14050...  Training loss: 1.7575...  0.0529 sec/batch\n",
      "Epoch: 8/20...  Training Step: 14100...  Training loss: 1.7671...  0.0537 sec/batch\n",
      "Epoch: 8/20...  Training Step: 14150...  Training loss: 1.8254...  0.0528 sec/batch\n",
      "Epoch: 8/20...  Training Step: 14200...  Training loss: 1.7165...  0.0564 sec/batch\n",
      "Epoch: 8/20...  Training Step: 14250...  Training loss: 1.7676...  0.0568 sec/batch\n",
      "Epoch: 8/20...  Training Step: 14300...  Training loss: 1.7572...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14350...  Training loss: 1.8500...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14400...  Training loss: 1.6823...  0.0551 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14450...  Training loss: 1.7643...  0.0526 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14500...  Training loss: 1.7273...  0.0536 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14550...  Training loss: 1.7992...  0.0576 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14600...  Training loss: 1.7495...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14650...  Training loss: 1.6918...  0.0532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14700...  Training loss: 1.8048...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14750...  Training loss: 1.8528...  0.0573 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14800...  Training loss: 1.8609...  0.0536 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 14850...  Training loss: 1.7998...  0.0582 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14900...  Training loss: 1.8372...  0.0534 sec/batch\n",
      "Epoch: 9/20...  Training Step: 14950...  Training loss: 1.8067...  0.0540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15000...  Training loss: 1.6907...  0.0578 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15050...  Training loss: 1.7446...  0.0563 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15100...  Training loss: 1.7953...  0.0575 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15150...  Training loss: 1.7273...  0.0593 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15200...  Training loss: 1.8483...  0.0539 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15250...  Training loss: 1.8002...  0.0529 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15300...  Training loss: 1.9102...  0.0542 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15350...  Training loss: 1.8625...  0.0536 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15400...  Training loss: 1.9272...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15450...  Training loss: 1.7678...  0.0566 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15500...  Training loss: 1.8281...  0.0535 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15550...  Training loss: 1.8033...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15600...  Training loss: 1.7868...  0.0552 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15650...  Training loss: 1.9524...  0.0527 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15700...  Training loss: 1.7899...  0.0535 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15750...  Training loss: 1.7990...  0.0574 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15800...  Training loss: 1.8087...  0.0570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15850...  Training loss: 1.7153...  0.0536 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15900...  Training loss: 1.7299...  0.0553 sec/batch\n",
      "Epoch: 9/20...  Training Step: 15950...  Training loss: 1.6655...  0.0574 sec/batch\n",
      "Epoch: 9/20...  Training Step: 16000...  Training loss: 1.8148...  0.0536 sec/batch\n",
      "Epoch: 9/20...  Training Step: 16050...  Training loss: 1.7389...  0.0567 sec/batch\n",
      "Epoch: 9/20...  Training Step: 16100...  Training loss: 1.8196...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16150...  Training loss: 1.6434...  0.0575 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16200...  Training loss: 1.8877...  0.0556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16250...  Training loss: 1.8186...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16300...  Training loss: 1.8640...  0.0554 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16350...  Training loss: 1.8178...  0.0557 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16400...  Training loss: 1.8724...  0.0568 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16450...  Training loss: 1.8342...  0.0527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16500...  Training loss: 1.7685...  0.0523 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16550...  Training loss: 1.7205...  0.0563 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16600...  Training loss: 1.7790...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16650...  Training loss: 1.7850...  0.0537 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16700...  Training loss: 1.6969...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16750...  Training loss: 1.7140...  0.0582 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16800...  Training loss: 1.8777...  0.0561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16850...  Training loss: 1.7224...  0.0601 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16900...  Training loss: 1.7725...  0.0551 sec/batch\n",
      "Epoch: 10/20...  Training Step: 16950...  Training loss: 1.7915...  0.0548 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17000...  Training loss: 1.7810...  0.0555 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17050...  Training loss: 1.9560...  0.0556 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17100...  Training loss: 1.7742...  0.0584 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17150...  Training loss: 1.7614...  0.0549 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17200...  Training loss: 1.8899...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17250...  Training loss: 1.7528...  0.0576 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17300...  Training loss: 1.7647...  0.0574 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17350...  Training loss: 1.7599...  0.0553 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17400...  Training loss: 1.6959...  0.0569 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17450...  Training loss: 1.8213...  0.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17500...  Training loss: 1.6295...  0.0596 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17550...  Training loss: 1.8660...  0.0581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17600...  Training loss: 1.7195...  0.0592 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17650...  Training loss: 1.8014...  0.0525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17700...  Training loss: 1.8741...  0.0536 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17750...  Training loss: 1.8592...  0.0594 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17800...  Training loss: 1.8816...  0.0572 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17850...  Training loss: 1.7999...  0.0569 sec/batch\n",
      "Epoch: 10/20...  Training Step: 17900...  Training loss: 1.6542...  0.0535 sec/batch\n",
      "Epoch: 11/20...  Training Step: 17950...  Training loss: 1.8391...  0.0528 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18000...  Training loss: 1.8200...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18050...  Training loss: 1.8172...  0.0572 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18100...  Training loss: 1.7596...  0.0538 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18150...  Training loss: 1.6737...  0.0574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18200...  Training loss: 1.8857...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18250...  Training loss: 1.6067...  0.0532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18300...  Training loss: 1.8360...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18350...  Training loss: 1.7391...  0.0558 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18400...  Training loss: 1.6648...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18450...  Training loss: 1.6967...  0.0579 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18500...  Training loss: 1.7062...  0.0579 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18550...  Training loss: 1.8425...  0.0567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18600...  Training loss: 1.8995...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18650...  Training loss: 1.8067...  0.0553 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18700...  Training loss: 1.8081...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18750...  Training loss: 1.8030...  0.0534 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18800...  Training loss: 1.7696...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18850...  Training loss: 1.9041...  0.0527 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18900...  Training loss: 1.8589...  0.0539 sec/batch\n",
      "Epoch: 11/20...  Training Step: 18950...  Training loss: 1.7775...  0.0559 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19000...  Training loss: 1.7526...  0.0532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19050...  Training loss: 1.7501...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19100...  Training loss: 1.8129...  0.0572 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19150...  Training loss: 1.7410...  0.0536 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19200...  Training loss: 1.8657...  0.0526 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19250...  Training loss: 1.7539...  0.0571 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19300...  Training loss: 1.6474...  0.0531 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19350...  Training loss: 2.0076...  0.0549 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19400...  Training loss: 1.9674...  0.0580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19450...  Training loss: 1.8365...  0.0557 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19500...  Training loss: 1.7612...  0.0560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19550...  Training loss: 1.7348...  0.0567 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19600...  Training loss: 1.8383...  0.0582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 19650...  Training loss: 1.6836...  0.0564 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 19700...  Training loss: 2.0123...  0.0577 sec/batch\n",
      "Epoch: 12/20...  Training Step: 19750...  Training loss: 1.8515...  0.0570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 19800...  Training loss: 1.6793...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 19850...  Training loss: 1.9866...  0.0554 sec/batch\n",
      "Epoch: 12/20...  Training Step: 19900...  Training loss: 1.8985...  0.0583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 19950...  Training loss: 1.7675...  0.0582 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20000...  Training loss: 1.6944...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20050...  Training loss: 1.7886...  0.0527 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20100...  Training loss: 1.7885...  0.0536 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20150...  Training loss: 1.7040...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20200...  Training loss: 1.7522...  0.0576 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20250...  Training loss: 1.6167...  0.0533 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20300...  Training loss: 1.6550...  0.0570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20350...  Training loss: 1.7467...  0.0561 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20400...  Training loss: 1.7051...  0.0546 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20450...  Training loss: 1.7284...  0.0596 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20500...  Training loss: 1.9598...  0.0550 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20550...  Training loss: 1.7674...  0.0597 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20600...  Training loss: 1.7933...  0.0535 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20650...  Training loss: 1.7465...  0.0562 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20700...  Training loss: 1.7127...  0.0553 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20750...  Training loss: 1.8246...  0.0586 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20800...  Training loss: 1.6786...  0.0551 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20850...  Training loss: 1.8154...  0.0565 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20900...  Training loss: 1.9138...  0.0555 sec/batch\n",
      "Epoch: 12/20...  Training Step: 20950...  Training loss: 1.8351...  0.0587 sec/batch\n",
      "Epoch: 12/20...  Training Step: 21000...  Training loss: 1.7633...  0.0569 sec/batch\n",
      "Epoch: 12/20...  Training Step: 21050...  Training loss: 1.9229...  0.0572 sec/batch\n",
      "Epoch: 12/20...  Training Step: 21100...  Training loss: 1.7373...  0.0599 sec/batch\n",
      "Epoch: 12/20...  Training Step: 21150...  Training loss: 1.6915...  0.0522 sec/batch\n",
      "Epoch: 12/20...  Training Step: 21200...  Training loss: 1.7757...  0.0590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 21250...  Training loss: 1.8485...  0.0538 sec/batch\n",
      "Epoch: 12/20...  Training Step: 21300...  Training loss: 1.7046...  0.0523 sec/batch\n",
      "Epoch: 12/20...  Training Step: 21350...  Training loss: 1.7428...  0.0531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 21400...  Training loss: 1.6977...  0.0583 sec/batch\n",
      "Epoch: 12/20...  Training Step: 21450...  Training loss: 1.6465...  0.0567 sec/batch\n",
      "Epoch: 13/20...  Training Step: 21500...  Training loss: 1.6195...  0.0527 sec/batch\n",
      "Epoch: 13/20...  Training Step: 21550...  Training loss: 1.7405...  0.0528 sec/batch\n",
      "Epoch: 13/20...  Training Step: 21600...  Training loss: 1.7337...  0.0539 sec/batch\n",
      "Epoch: 13/20...  Training Step: 21650...  Training loss: 1.8050...  0.0608 sec/batch\n",
      "Epoch: 13/20...  Training Step: 21700...  Training loss: 1.8192...  0.0593 sec/batch\n",
      "Epoch: 13/20...  Training Step: 21750...  Training loss: 1.8822...  0.0559 sec/batch\n",
      "Epoch: 13/20...  Training Step: 21800...  Training loss: 1.6827...  0.0534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 21850...  Training loss: 1.7025...  0.0538 sec/batch\n",
      "Epoch: 13/20...  Training Step: 21900...  Training loss: 1.7663...  0.0531 sec/batch\n",
      "Epoch: 13/20...  Training Step: 21950...  Training loss: 1.8233...  0.0542 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22000...  Training loss: 1.7253...  0.0536 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22050...  Training loss: 1.7208...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22100...  Training loss: 1.6520...  0.0542 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22150...  Training loss: 1.6971...  0.0552 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22200...  Training loss: 1.7567...  0.0521 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22250...  Training loss: 1.6190...  0.0533 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22300...  Training loss: 1.7065...  0.0532 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22350...  Training loss: 1.7133...  0.0579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22400...  Training loss: 1.7975...  0.0535 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22450...  Training loss: 1.7531...  0.0546 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22500...  Training loss: 1.9044...  0.0576 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22550...  Training loss: 1.6953...  0.0525 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22600...  Training loss: 1.8451...  0.0584 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22650...  Training loss: 1.7644...  0.0578 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22700...  Training loss: 1.7238...  0.0587 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22750...  Training loss: 1.7164...  0.0598 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22800...  Training loss: 1.8522...  0.0570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22850...  Training loss: 1.6587...  0.0573 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22900...  Training loss: 1.8020...  0.0553 sec/batch\n",
      "Epoch: 13/20...  Training Step: 22950...  Training loss: 1.6999...  0.0536 sec/batch\n",
      "Epoch: 13/20...  Training Step: 23000...  Training loss: 1.7962...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 23050...  Training loss: 1.7266...  0.0579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 23100...  Training loss: 1.7600...  0.0554 sec/batch\n",
      "Epoch: 13/20...  Training Step: 23150...  Training loss: 1.6003...  0.0539 sec/batch\n",
      "Epoch: 13/20...  Training Step: 23200...  Training loss: 1.7967...  0.0599 sec/batch\n",
      "Epoch: 13/20...  Training Step: 23250...  Training loss: 1.8399...  0.0564 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23300...  Training loss: 1.7010...  0.0548 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23350...  Training loss: 1.5687...  0.0586 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23400...  Training loss: 1.8062...  0.0580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23450...  Training loss: 1.7212...  0.0578 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23500...  Training loss: 1.7520...  0.0564 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23550...  Training loss: 1.6494...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23600...  Training loss: 1.8455...  0.0532 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23650...  Training loss: 1.7510...  0.0539 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23700...  Training loss: 1.7258...  0.0555 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23750...  Training loss: 1.7018...  0.0531 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23800...  Training loss: 1.7201...  0.0545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23850...  Training loss: 1.6755...  0.0583 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23900...  Training loss: 1.6533...  0.0598 sec/batch\n",
      "Epoch: 14/20...  Training Step: 23950...  Training loss: 1.7864...  0.0578 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24000...  Training loss: 1.8566...  0.0573 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24050...  Training loss: 1.7401...  0.0577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24100...  Training loss: 1.7299...  0.0551 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24150...  Training loss: 1.8300...  0.0577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24200...  Training loss: 1.9470...  0.0594 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24250...  Training loss: 1.8489...  0.0583 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24300...  Training loss: 1.8194...  0.0536 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24350...  Training loss: 1.8105...  0.0583 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24400...  Training loss: 1.8525...  0.0572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24450...  Training loss: 1.7514...  0.0564 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24500...  Training loss: 1.7832...  0.0555 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 24550...  Training loss: 1.7823...  0.0574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24600...  Training loss: 1.8236...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24650...  Training loss: 1.6900...  0.0529 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24700...  Training loss: 1.6795...  0.0599 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24750...  Training loss: 1.7965...  0.0569 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24800...  Training loss: 1.7467...  0.0604 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24850...  Training loss: 1.6667...  0.0591 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24900...  Training loss: 1.6316...  0.0525 sec/batch\n",
      "Epoch: 14/20...  Training Step: 24950...  Training loss: 1.7028...  0.0569 sec/batch\n",
      "Epoch: 14/20...  Training Step: 25000...  Training loss: 1.7937...  0.0574 sec/batch\n",
      "Epoch: 14/20...  Training Step: 25050...  Training loss: 1.6833...  0.0560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25100...  Training loss: 1.8016...  0.0579 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25150...  Training loss: 1.5829...  0.0584 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25200...  Training loss: 1.6766...  0.0570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25250...  Training loss: 1.7506...  0.0534 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25300...  Training loss: 1.7643...  0.0588 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25350...  Training loss: 1.7428...  0.0592 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25400...  Training loss: 1.8725...  0.0599 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25450...  Training loss: 1.7226...  0.0584 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25500...  Training loss: 1.9041...  0.0533 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25550...  Training loss: 1.7085...  0.0538 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25600...  Training loss: 1.7762...  0.0564 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25650...  Training loss: 1.7138...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25700...  Training loss: 1.8603...  0.0534 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25750...  Training loss: 1.6734...  0.0564 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25800...  Training loss: 1.7877...  0.0569 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25850...  Training loss: 1.6903...  0.0539 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25900...  Training loss: 1.7885...  0.0534 sec/batch\n",
      "Epoch: 15/20...  Training Step: 25950...  Training loss: 1.8218...  0.0549 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26000...  Training loss: 1.6242...  0.0591 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26050...  Training loss: 1.6592...  0.0573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26100...  Training loss: 1.8669...  0.0533 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26150...  Training loss: 1.7612...  0.0603 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26200...  Training loss: 1.7896...  0.0531 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26250...  Training loss: 1.8473...  0.0552 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26300...  Training loss: 1.7267...  0.0533 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26350...  Training loss: 1.7862...  0.0581 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26400...  Training loss: 1.7145...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26450...  Training loss: 1.8758...  0.0573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26500...  Training loss: 1.8108...  0.0578 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26550...  Training loss: 1.6291...  0.0579 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26600...  Training loss: 1.7441...  0.0571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26650...  Training loss: 1.6610...  0.0535 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26700...  Training loss: 1.6087...  0.0553 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26750...  Training loss: 1.6831...  0.0545 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26800...  Training loss: 1.7461...  0.0526 sec/batch\n",
      "Epoch: 15/20...  Training Step: 26850...  Training loss: 1.8289...  0.0536 sec/batch\n",
      "Epoch: 16/20...  Training Step: 26900...  Training loss: 1.8557...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 26950...  Training loss: 1.7622...  0.0597 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27000...  Training loss: 1.8420...  0.0546 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27050...  Training loss: 1.6607...  0.0552 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27100...  Training loss: 1.7068...  0.0571 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27150...  Training loss: 1.7745...  0.0542 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27200...  Training loss: 1.6106...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27250...  Training loss: 1.8060...  0.0586 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27300...  Training loss: 1.7151...  0.0531 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27350...  Training loss: 1.8860...  0.0578 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27400...  Training loss: 1.8059...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27450...  Training loss: 1.7843...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27500...  Training loss: 1.7885...  0.0577 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27550...  Training loss: 1.7658...  0.0566 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27600...  Training loss: 1.7101...  0.0558 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27650...  Training loss: 1.7934...  0.0591 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27700...  Training loss: 1.7596...  0.0601 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27750...  Training loss: 1.8165...  0.0555 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27800...  Training loss: 1.7855...  0.0532 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27850...  Training loss: 1.6400...  0.0601 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27900...  Training loss: 1.6707...  0.0534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 27950...  Training loss: 1.7613...  0.0540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28000...  Training loss: 1.7152...  0.0535 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28050...  Training loss: 1.7528...  0.0574 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28100...  Training loss: 1.6459...  0.0551 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28150...  Training loss: 1.6850...  0.0541 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28200...  Training loss: 1.8869...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28250...  Training loss: 1.8197...  0.0585 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28300...  Training loss: 1.5735...  0.0572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28350...  Training loss: 1.8698...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28400...  Training loss: 1.7383...  0.0548 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28450...  Training loss: 1.7932...  0.0579 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28500...  Training loss: 1.7968...  0.0579 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28550...  Training loss: 1.7241...  0.0568 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28600...  Training loss: 1.7736...  0.0574 sec/batch\n",
      "Epoch: 16/20...  Training Step: 28650...  Training loss: 1.7253...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 28700...  Training loss: 1.8048...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 28750...  Training loss: 1.6468...  0.0525 sec/batch\n",
      "Epoch: 17/20...  Training Step: 28800...  Training loss: 1.8274...  0.0555 sec/batch\n",
      "Epoch: 17/20...  Training Step: 28850...  Training loss: 1.7863...  0.0598 sec/batch\n",
      "Epoch: 17/20...  Training Step: 28900...  Training loss: 1.7873...  0.0571 sec/batch\n",
      "Epoch: 17/20...  Training Step: 28950...  Training loss: 1.7017...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29000...  Training loss: 1.7956...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29050...  Training loss: 1.7506...  0.0576 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29100...  Training loss: 1.9432...  0.0575 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29150...  Training loss: 1.6898...  0.0573 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29200...  Training loss: 1.7692...  0.0549 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29250...  Training loss: 1.6959...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29300...  Training loss: 1.7427...  0.0570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29350...  Training loss: 1.7939...  0.0526 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 29400...  Training loss: 1.8221...  0.0546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29450...  Training loss: 1.8874...  0.0574 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29500...  Training loss: 1.9169...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29550...  Training loss: 1.7483...  0.0545 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29600...  Training loss: 1.8436...  0.0534 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29650...  Training loss: 1.6681...  0.0559 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29700...  Training loss: 1.7720...  0.0548 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29750...  Training loss: 1.7068...  0.0574 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29800...  Training loss: 1.7992...  0.0591 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29850...  Training loss: 1.7315...  0.0586 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29900...  Training loss: 1.7415...  0.0536 sec/batch\n",
      "Epoch: 17/20...  Training Step: 29950...  Training loss: 1.6943...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 30000...  Training loss: 1.7961...  0.0554 sec/batch\n",
      "Epoch: 17/20...  Training Step: 30050...  Training loss: 1.6016...  0.0575 sec/batch\n",
      "Epoch: 17/20...  Training Step: 30100...  Training loss: 1.7347...  0.0576 sec/batch\n",
      "Epoch: 17/20...  Training Step: 30150...  Training loss: 1.7315...  0.0606 sec/batch\n",
      "Epoch: 17/20...  Training Step: 30200...  Training loss: 1.8193...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 30250...  Training loss: 1.7917...  0.0526 sec/batch\n",
      "Epoch: 17/20...  Training Step: 30300...  Training loss: 1.7185...  0.0557 sec/batch\n",
      "Epoch: 17/20...  Training Step: 30350...  Training loss: 1.7030...  0.0556 sec/batch\n",
      "Epoch: 17/20...  Training Step: 30400...  Training loss: 1.6066...  0.0537 sec/batch\n",
      "Epoch: 18/20...  Training Step: 30450...  Training loss: 1.7124...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 30500...  Training loss: 1.7910...  0.0576 sec/batch\n",
      "Epoch: 18/20...  Training Step: 30550...  Training loss: 1.7337...  0.0572 sec/batch\n",
      "Epoch: 18/20...  Training Step: 30600...  Training loss: 1.6706...  0.0546 sec/batch\n",
      "Epoch: 18/20...  Training Step: 30650...  Training loss: 1.6880...  0.0521 sec/batch\n",
      "Epoch: 18/20...  Training Step: 30700...  Training loss: 1.6353...  0.0527 sec/batch\n",
      "Epoch: 18/20...  Training Step: 30750...  Training loss: 1.7036...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 30800...  Training loss: 1.6337...  0.0595 sec/batch\n",
      "Epoch: 18/20...  Training Step: 30850...  Training loss: 1.6761...  0.0538 sec/batch\n",
      "Epoch: 18/20...  Training Step: 30900...  Training loss: 1.7103...  0.0531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 30950...  Training loss: 1.7840...  0.0561 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31000...  Training loss: 1.8000...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31050...  Training loss: 1.7659...  0.0552 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31100...  Training loss: 1.6746...  0.0534 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31150...  Training loss: 1.8653...  0.0584 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31200...  Training loss: 1.6287...  0.0528 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31250...  Training loss: 1.8449...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31300...  Training loss: 1.6157...  0.0562 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31350...  Training loss: 1.8287...  0.0526 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31400...  Training loss: 1.6483...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31450...  Training loss: 1.7178...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31500...  Training loss: 1.8914...  0.0581 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31550...  Training loss: 1.7750...  0.0564 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31600...  Training loss: 1.7283...  0.0557 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31650...  Training loss: 1.7836...  0.0553 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31700...  Training loss: 1.7837...  0.0538 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31750...  Training loss: 1.6086...  0.0569 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31800...  Training loss: 1.8635...  0.0559 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31850...  Training loss: 1.7701...  0.0555 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31900...  Training loss: 1.7156...  0.0556 sec/batch\n",
      "Epoch: 18/20...  Training Step: 31950...  Training loss: 1.6492...  0.0581 sec/batch\n",
      "Epoch: 18/20...  Training Step: 32000...  Training loss: 1.6422...  0.0566 sec/batch\n",
      "Epoch: 18/20...  Training Step: 32050...  Training loss: 1.6253...  0.0554 sec/batch\n",
      "Epoch: 18/20...  Training Step: 32100...  Training loss: 1.8245...  0.0532 sec/batch\n",
      "Epoch: 18/20...  Training Step: 32150...  Training loss: 1.6909...  0.0590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 32200...  Training loss: 1.5760...  0.0524 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32250...  Training loss: 1.7634...  0.0526 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32300...  Training loss: 1.7994...  0.0562 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32350...  Training loss: 1.6868...  0.0547 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32400...  Training loss: 1.7181...  0.0578 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32450...  Training loss: 1.8035...  0.0535 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32500...  Training loss: 1.7623...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32550...  Training loss: 1.6913...  0.0556 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32600...  Training loss: 1.6934...  0.0536 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32650...  Training loss: 1.8597...  0.0594 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32700...  Training loss: 1.7192...  0.0563 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32750...  Training loss: 1.6702...  0.0570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32800...  Training loss: 1.6105...  0.0578 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32850...  Training loss: 1.7415...  0.0583 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32900...  Training loss: 1.6361...  0.0531 sec/batch\n",
      "Epoch: 19/20...  Training Step: 32950...  Training loss: 1.7986...  0.0562 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33000...  Training loss: 1.5436...  0.0576 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33050...  Training loss: 1.7067...  0.0583 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33100...  Training loss: 1.7091...  0.0528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33150...  Training loss: 1.7892...  0.0535 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33200...  Training loss: 1.7792...  0.0525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33250...  Training loss: 1.6300...  0.0561 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33300...  Training loss: 1.6345...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33350...  Training loss: 1.7347...  0.0552 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33400...  Training loss: 1.9473...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33450...  Training loss: 1.6468...  0.0533 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33500...  Training loss: 1.8857...  0.0534 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33550...  Training loss: 1.8843...  0.0571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33600...  Training loss: 1.6261...  0.0566 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33650...  Training loss: 1.7666...  0.0543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33700...  Training loss: 1.6290...  0.0557 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33750...  Training loss: 1.6248...  0.0529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33800...  Training loss: 1.8432...  0.0534 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33850...  Training loss: 1.5062...  0.0538 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33900...  Training loss: 1.7806...  0.0534 sec/batch\n",
      "Epoch: 19/20...  Training Step: 33950...  Training loss: 1.8280...  0.0572 sec/batch\n",
      "Epoch: 19/20...  Training Step: 34000...  Training loss: 1.6505...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34050...  Training loss: 1.7673...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34100...  Training loss: 1.6767...  0.0559 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34150...  Training loss: 1.7642...  0.0594 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34200...  Training loss: 1.6220...  0.0533 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 34250...  Training loss: 1.7387...  0.0573 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34300...  Training loss: 1.8364...  0.0599 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34350...  Training loss: 1.6159...  0.0553 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34400...  Training loss: 1.6049...  0.0571 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34450...  Training loss: 1.6911...  0.0534 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34500...  Training loss: 1.6979...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34550...  Training loss: 1.7935...  0.0550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34600...  Training loss: 1.6797...  0.0545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34650...  Training loss: 1.6567...  0.0539 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34700...  Training loss: 1.7191...  0.0539 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34750...  Training loss: 1.7842...  0.0565 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34800...  Training loss: 1.7931...  0.0529 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34850...  Training loss: 1.5819...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34900...  Training loss: 1.6754...  0.0537 sec/batch\n",
      "Epoch: 20/20...  Training Step: 34950...  Training loss: 1.7549...  0.0581 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35000...  Training loss: 1.7374...  0.0579 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35050...  Training loss: 1.7734...  0.0586 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35100...  Training loss: 1.7102...  0.0534 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35150...  Training loss: 1.8541...  0.0587 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35200...  Training loss: 1.8504...  0.0568 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35250...  Training loss: 1.7143...  0.0609 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35300...  Training loss: 1.7599...  0.0568 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35350...  Training loss: 1.6786...  0.0569 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35400...  Training loss: 1.7925...  0.0534 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35450...  Training loss: 1.6488...  0.0519 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35500...  Training loss: 1.7632...  0.0558 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35550...  Training loss: 1.7600...  0.0533 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35600...  Training loss: 1.7735...  0.0541 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35650...  Training loss: 1.7025...  0.0538 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35700...  Training loss: 1.7916...  0.0586 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35750...  Training loss: 1.7192...  0.0528 sec/batch\n",
      "Epoch: 20/20...  Training Step: 35800...  Training loss: 1.7556...  0.0560 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Print losses every N interations\n",
    "print_every_n = 50\n",
    "\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i35820_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i34000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i34200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i34400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i34600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i34800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35820_l128.ckpt\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i35820_l128.ckpt'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fartonit. Do t  do t truuit. \n",
      "Ata ishte prpara porsa njerzniva nj \n",
      "gjithuk po \n",
      "prgjagja, n nj shpejste nuk e prgjigjur t \n",
      "\n",
      "ndindur. \n",
      "Sipis thoshte \n",
      "tepr t ndrsrete e pasini \n",
      "t tij, t shtrenat e \n",
      "pashtresin. Ajo, dentiti n nj kmeshn i \n",
      "porsuenin dy t thosht t \n",
      "mendohet t kujdohesht, tani ndshpar n kokn sikur \n",
      "nga \n",
      "ndjyeshi prej nga at. \n",
      "Ai e tha ato t trulle t ndanuar katrto me pas ti\n",
      "t'D dhe t pare \n",
      "matusn pjes... \n",
      "\n",
      "Ñ An e \n",
      "t gjith pa dashur pas po kalonit dhe \n",
      "ndihmni me nj t, ku ngrihej \n",
      "e ns shkurtet. \n",
      "\n",
      "ÑPo t koh pas pak t thoshte njrn aty njortoseshesh \n",
      "i theti, npas. \n",
      "Ata pastaj i shkoa t madh depatis dhiu \n",
      "pas tyre, por m ardhur te ditat dhe \n",
      "\n",
      "kishte prndojt se m ndeitire \n",
      "pika \n",
      "me njn ndrtesit. \n",
      "ÑPo prperia e till tryeza e tyre diti tani meshurit \n",
      "nj mbando t tjern se n nj gjekta prpara m tepo nuk \n",
      "i prerjen qt, \n",
      "pos t  thoshte \n",
      "ndrgjetja dhe nj dshir. \n",
      "ÑPranira t ndojunja e prprtitjen.iNj me \n",
      "marretime nj nn nn dit t marr, \n",
      "donin pr t mundur n sy mbni \n",
      "me t po ishte me shenj e trhame njri t \n",
      "ndiqesh e kndej ndoshta e prgjigjem pes \n",
      "n ngjajt e kuptueshkpiheshin \n",
      "tan s \n",
      "t thrt t trihen nga njapes prpera ndiej n tamet, po ndshtin do \n",
      "pr te ndryshe \n",
      "shkend shprtithar duhet t pak \n",
      "t t gjithuos shu\n",
      "mur pas saj. Ato npr se pa \n",
      "n potalluar n shkelu nj \n",
      "ndor n prthente nje gjerhejtisesh.paku, nns, dhe sidisi t me pa t \n",
      "pran, t shkonte me tr t t paninat, n shpejt t sills, \n",
      "se sesin \n",
      "e pashpejt po porte e dsajtur \n",
      "pr t djamit,, s'unim pat e katr prprshet dhe pas t gjet me t porsakill prej \n",
      "me tra t gjat. Prej \n",
      "nga portas n \n",
      "\n",
      "\f",
      "\n",
      "\n",
      "\f",
      "\n",
      "dhe pastaj ngjiti dallgote t ndeullimt do t kishin shemist mi e kaloneshn. \n",
      "Nj mmiri ndoshta nj ta \n",
      "po mundim ndoshta nj kuptoi t m dukt t pakishtint e si pr vete, \n",
      "si t kujtoi mbahtur e tij. \n",
      "\n",
      "Ñ Si prinj plodui t pjats pas ndonus si kam syr t shmaktorin \n",
      "e ve dkutet \n",
      "e kanurt d\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
